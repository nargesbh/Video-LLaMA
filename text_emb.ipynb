{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmadi/miniconda3/envs/video-ir/lib/python3.10/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n",
      "/home/ahmadi/miniconda3/envs/video-ir/lib/python3.10/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/home/ahmadi/miniconda3/envs/video-ir/lib/python3.10/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Adapted from: https://github.com/Vision-CAIR/MiniGPT-4/blob/main/demo.py\n",
    "\"\"\"\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import gradio as gr\n",
    "\n",
    "from video_llama.common.config import Config\n",
    "from video_llama.common.dist_utils import get_rank\n",
    "from video_llama.common.registry import registry\n",
    "from video_llama.conversation.conversation_video import Chat, Conversation, default_conversation,SeparatorStyle,conv_llava_llama_2\n",
    "import decord\n",
    "decord.bridge.set_bridge('torch')\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "#%%\n",
    "# imports modules for registration\n",
    "from video_llama.datasets.builders import *\n",
    "from video_llama.models import *\n",
    "from video_llama.processors import *\n",
    "from video_llama.runners import *\n",
    "from video_llama.tasks import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Demo\")\n",
    "    parser.add_argument(\"--cfg-path\", default='eval_configs/video_llama_eval_withaudio.yaml', help=\"path to configuration file.\")\n",
    "    parser.add_argument(\"--gpu-id\", type=int, default=0, help=\"specify the gpu to load the model.\")\n",
    "    parser.add_argument(\"--model_type\", type=str, default='vicuna', help=\"The type of LLM\")\n",
    "    parser.add_argument(\n",
    "        \"--options\",\n",
    "        nargs=\"+\",\n",
    "        help=\"override some settings in the used config, the key-value pair \"\n",
    "        \"in xxx=yyy format will be merged into config file (deprecate), \"\n",
    "        \"change to --cfg-options instead.\",\n",
    "    )\n",
    "    # args = parser.parse_args()\n",
    "    args, remaining_args = parser.parse_known_args()\n",
    "    return args, remaining_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_llama_sentence_embeddings(llama, tokenizer, texts: str | List[str], avg=True, normalize=True):\n",
    "    inps = tokenizer(texts, return_tensors=\"pt\", padding=True).to(llama.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embs = llama(**inps)[\"last_hidden_state\"]\n",
    "\n",
    "    \n",
    "    return embs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    df_dir: str = \"/home/ahmadi/video-ir/dataset/filtered_captions.csv\",\n",
    "    save_dir_path: str = \"/home/ahmadi/video-ir/dataset/llama_data/after_pooling/trainVal_embeddings/text/20cap/llama_txt_embedding_AveragePooling\",\n",
    "    First_element: bool = False,\n",
    "    Max_pooling: bool = False,\n",
    "    Average_pooling: bool = True,\n",
    "    Sum: bool = False    \n",
    "    ):\n",
    "    # Model Initialization\n",
    "    print('Initializing Chat')\n",
    "    args, remaining_args = parse_args()\n",
    "    cfg = Config(args)\n",
    "\n",
    "    model_config = cfg.model_cfg\n",
    "    model_config.device_8bit = args.gpu_id\n",
    "\n",
    "    model_cls = registry.get_model_class(model_config.arch)\n",
    "    model = model_cls.from_config(model_config).to('cuda:{}'.format(args.gpu_id))\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    df = pd.read_csv(df_dir)\n",
    "        \n",
    "    vid_names = df[\"video_id\"]\n",
    "    captions = df['sentence']\n",
    "\n",
    "    tokenizer = model.llama_tokenizer\n",
    "\n",
    "\n",
    "    for i in tqdm(range(len(captions))):\n",
    "        \n",
    "        embeddings = compute_llama_sentence_embeddings(model.llama_model.model, tokenizer, captions[i])\n",
    "        embeddings = embeddings.float()\n",
    "        embeddings = embeddings.cpu().detach().numpy()\n",
    "        \n",
    "        # print(captions[i])\n",
    "        # print(embeddings.shape)\n",
    "        \n",
    "        if First_element:\n",
    "            embeddings = embeddings[0][0]\n",
    "                \n",
    "        elif Max_pooling:\n",
    "            embeddings = np.max(embeddings[0], axis=0)\n",
    "                \n",
    "        elif Average_pooling:\n",
    "            embeddings = np.min(embeddings[0], axis=0)\n",
    "                \n",
    "        elif Sum:\n",
    "            embeddings = np.sum(embeddings[0], axis=0)\n",
    "        \n",
    "        # embeddings = embeddings[0]\n",
    "        # print(embeddings.shape)\n",
    "        p_init = save_dir_path + \"/\" + vid_names[i] \n",
    "        \n",
    "        for j in range(20):\n",
    "            if os.path.isfile(p_init+\"-\"+ j + \".npy\") == False:\n",
    "                np.save(p_init+\"-\"+ j + \".npy\", embeddings)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Chat\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading VIT\n",
      "Loading VIT Done\n",
      "Loading Q-Former\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d18e5b0e8c4c7cb4cc21d55a9e24e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing audio encoder from /home/ahmadi/video-ir/video-LLAMA/Image-bind ...\n",
      "audio encoder initialized.\n",
      "Load first Checkpoint: /home/ahmadi/video-ir/video-LLAMA/Video-LLaMA/finetune-vicuna7b-v2.pth\n",
      "Load second Checkpoint: /home/ahmadi/video-ir/video-LLAMA/Video-LLaMA/finetune_vicuna7b_audiobranch.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180000/180000 [1:24:33<00:00, 35.48it/s]\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First element is ok\n",
    "#sum is ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>video2960</td>\n",
       "      <td>a cartoon animals runs through an ice cave in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>video2960</td>\n",
       "      <td>a cartoon character runs around inside of a vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>video2960</td>\n",
       "      <td>a character is running in the snow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>video2960</td>\n",
       "      <td>a person plays a video game centered around ic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>video2960</td>\n",
       "      <td>a person plays online and records themselves</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179995</th>\n",
       "      <td>video8600</td>\n",
       "      <td>shows a man in a red sweeter and white shirt  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179996</th>\n",
       "      <td>video8600</td>\n",
       "      <td>a man explains how to save money using careful...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179997</th>\n",
       "      <td>video8600</td>\n",
       "      <td>a person with maroon tshirt speaks in the news...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179998</th>\n",
       "      <td>video8600</td>\n",
       "      <td>the man in a purple sweater is giving a news s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179999</th>\n",
       "      <td>video8600</td>\n",
       "      <td>a person is speaking about something in a live...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         video_id                                           sentence\n",
       "0       video2960  a cartoon animals runs through an ice cave in ...\n",
       "1       video2960  a cartoon character runs around inside of a vi...\n",
       "2       video2960                 a character is running in the snow\n",
       "3       video2960  a person plays a video game centered around ic...\n",
       "4       video2960       a person plays online and records themselves\n",
       "...           ...                                                ...\n",
       "179995  video8600  shows a man in a red sweeter and white shirt  ...\n",
       "179996  video8600  a man explains how to save money using careful...\n",
       "179997  video8600  a person with maroon tshirt speaks in the news...\n",
       "179998  video8600  the man in a purple sweater is giving a news s...\n",
       "179999  video8600  a person is speaking about something in a live...\n",
       "\n",
       "[180000 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = '/home/ahmadi/video-ir/dataset/filtered_captions.csv'\n",
    "df_2 = pd.read_csv(p)\n",
    "df_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "made_now = np.load('/home/ahmadi/video-ir/dataset/llama_data/before_pooling/trainVal_embeddings/text/video2960.npy')\n",
    "\n",
    "made_bef = np.load('/home/ahmadi/video-ir/dataset/llama_data/after_pooling/trainVal_embeddings/text/llama_txt_embedding_AveragePooling/video2960.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(made_now, made_bef, equal_nan=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "video-ir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
